{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "dataPath = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(dataPath + 'train.csv',index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "types = list(train)[1:]\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e90ecc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[types].mean().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "id                                                                           \n",
      "00001cee341fdb12     -1            -1       -1      -1      -1             -1\n",
      "0000247867823ef7     -1            -1       -1      -1      -1             -1\n",
      "00013b17ad220c46     -1            -1       -1      -1      -1             -1\n",
      "00017563c3f7919a     -1            -1       -1      -1      -1             -1\n",
      "00017695ad8997eb     -1            -1       -1      -1      -1             -1\n",
      "0.41770912224804785 % of test is labelled\n",
      "(223549, 7) (89186, 1)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(dataPath+'test.csv',index_col=0)\n",
    "test_labels = pd.read_csv(dataPath+'test_labels.csv',index_col=0)\n",
    "print(test_labels.head())\n",
    "labelled_test = test.join(test_labels)\n",
    "disclosed = labelled_test.toxic>-1\n",
    "print(disclosed .mean(),'% of test is labelled')\n",
    "train = train.append(labelled_test[disclosed])\n",
    "test = labelled_test[~disclosed][['comment_text']]\n",
    "print(train.shape,test.shape)\n",
    "train.to_csv(dataPath+'tc_train.csv')\n",
    "test.to_csv(dataPath+'tc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(dataPath + 'tc_train.csv',index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text\n",
       "id                                                                 \n",
       "00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(dataPath + 'tc_test.csv',index_col=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(file_name):\n",
    "    embeddings_index = {}\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            # remove white spaces and split\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) > 2:\n",
    "                embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = get_embeddings('crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "trans_table = str.maketrans({key: ' ' for key in string.digits + '\\r\\n' +\n",
    "                             string.punctuation.replace(\"\\'\",'')})\n",
    "def preprocess(text):\n",
    "    return ' '.join(text.lower().translate(trans_table).split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 5\n",
      "45259 top words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "UNKNOWN_PROXY = 'unknown'\n",
    "MIN_WORD_OCCURRENCE = 5\n",
    "\n",
    "train['comment_text'] = train.comment_text.apply(preprocess)\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", \n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(train.comment_text)\n",
    "\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(UNKNOWN_PROXY)\n",
    "print(len(top_words),'top words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 out of \"top_words\": \n",
      " ['upa', 'designed', 'dozen', 'bcs', 'huddersfield', 'lubbock', 'jfdwolff', 'ein', 'pico', 'dekker']\n",
      "\n",
      "Is \"unknown\" in top_words? \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "print('First 10 out of \"top_words\": \\n',list(top_words)[:10])\n",
    "print('\\nIs \"unknown\" in top_words? \\n','unknown' in top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x127af5fd0>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(train.comment_text)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('to', 2),\n",
       " ('of', 3),\n",
       " ('and', 4),\n",
       " ('a', 5),\n",
       " ('you', 6),\n",
       " ('i', 7),\n",
       " ('is', 8),\n",
       " ('that', 9),\n",
       " ('in', 10),\n",
       " ('it', 11),\n",
       " ('for', 12),\n",
       " ('this', 13),\n",
       " ('not', 14),\n",
       " ('on', 15),\n",
       " ('be', 16)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "list(word_index.items())[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data for Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input data for neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 sequences in `seq`:  [[676, 77, 1, 133, 130, 177, 30, 666, 4436, 11406, 1126, 85, 349, 51, 2184, 12587, 50, 6354, 15, 59, 2567, 148, 7, 2795, 33, 116, 1196, 15967, 2453, 4, 47, 60, 247, 1, 359, 31, 1, 41, 27, 143, 71, 3503, 89], [121402, 52, 2765, 13, 466, 3656, 71, 4530, 2696, 21, 93, 41, 968, 196]]\n",
      "\n",
      "Shape of `data`:  (223549, 50)\n",
      "\n",
      "First prepared text in `data`: [  676    77     1   133   130   177    30   666  4436 11406  1126    85\n",
      "   349    51  2184 12587    50  6354    15    59  2567   148     7  2795\n",
      "    33   116  1196 15967  2453     4    47    60   247     1   359    31\n",
      "     1    41    27   143    71  3503    89     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "seq = tokenizer.texts_to_sequences(train.comment_text)\n",
    "data = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                     truncating='post')\n",
    "with open(dataPath + 'toxic_data.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "print('\\nFirst 2 sequences in `seq`: ',seq[:2])\n",
    "print('\\nShape of `data`: ',data.shape)\n",
    "print('\\nFirst prepared text in `data`:',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_1\n",
      "word_2\n",
      "word_3\n",
      "enough\n",
      "enough\n"
     ]
    }
   ],
   "source": [
    "mlist=['word_1','word_2','word_3']\n",
    "moveIter=iter(mlist)\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(next(iter(embeddings_index.values())))\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index,embeddings_index):\n",
    "    nb_words = len(word_index) + 1 # +1 since min(word_index.values())=1\n",
    "    embedding_matrix = np.zeros((nb_words,embeddings_dim))\n",
    "    unknown = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None: unknown += 1\n",
    "        else: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160113 unknown words\n"
     ]
    }
   ],
   "source": [
    "def make_save_emb_layer(word_index,embeddings_index,layer_file_name):\n",
    "    embedding_matrix,unknown = get_embedding_matrix(word_index,embeddings_index)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],trainable=False)\n",
    "    with open(layer_file_name,'wb') as f: \n",
    "        pickle.dump(embedding_layer, f, -1)\n",
    "    return unknown\n",
    "\n",
    "EMBEDDING_LAYER_FILE = dataPath + 'toxic_embed_layer.pkl'\n",
    "print(make_save_emb_layer(word_index,embeddings_index,EMBEDDING_LAYER_FILE),\n",
    "      'unknown words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into new train and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "types = list(train)[1:]\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      " id\n",
      "0000997932d777bf    000000\n",
      "000103f0d9cfb60f    000000\n",
      "000113f07ec002fd    000000\n",
      "0001b41b1c6bb37e    000000\n",
      "0001d958c54c6e35    000000\n",
      "dtype: object\n",
      "\n",
      "Counts of labels: \n",
      " 000000    201081\n",
      "100000      7376\n",
      "101010      5732\n",
      "101000      2612\n",
      "100010      1754\n",
      "111010      1165\n",
      "101011       979\n",
      "111011       381\n",
      "001000       366\n",
      "000010       365\n",
      "100011       215\n",
      "100001       203\n",
      "001010       196\n",
      "101110       196\n",
      "111000       186\n",
      "100100       163\n",
      "111110        88\n",
      "101111        81\n",
      "000001        68\n",
      "101001        55\n",
      "111111        45\n",
      "110000        41\n",
      "000011        32\n",
      "000100        27\n",
      "100110        25\n",
      "001011        19\n",
      "101100        17\n",
      "110010        14\n",
      "110100        11\n",
      "100101        11\n",
      "111100         8\n",
      "111001         7\n",
      "110011         7\n",
      "110101         5\n",
      "rare           5\n",
      "000110         4\n",
      "100111         3\n",
      "110001         3\n",
      "001001         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert each vector of labels to the string\n",
    "labels = train[types].astype(str).apply(lambda x: ''.join(x),axis=1)\n",
    "print('Labels: \\n',labels.head())\n",
    "# aggregate rare combinations if any\n",
    "count = labels.value_counts()\n",
    "rare = count.index[count<=2]\n",
    "labels[np.isin(labels.values,rare)] = 'rare'\n",
    "print('\\nCounts of labels: \\n',labels.value_counts())\n",
    "train_index, val_index = train_test_split(list(range(data.shape[0])), test_size=0.2, \n",
    "                                      stratify = labels, random_state=0)\n",
    "# save train and validation indices for further calculations\n",
    "fname = dataPath + 'train_val_split.pkl'\n",
    "with open(fname, 'wb') as f: pickle.dump([train_index, val_index], f, -1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_LAYER_FILE, 'rb') as f: embedding_layer = pickle.load(f)\n",
    "with open(dataPath + 'toxic_data.pkl', 'rb') as f: data = pickle.load(f)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[train_index]\n",
    "X_test = data[val_index]\n",
    "\n",
    "y_train = train.iloc[train_index][['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "y_test = train.iloc[val_index][['toxic','severe_toxic','obscene','threat','insult','identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Embedding,Input,Dropout,Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, Flatten,LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x = LSTM(10, return_sequences=True)(x)\n",
    "    x = Conv1D(5, kernel_size=2, padding=\"valid\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/capstone/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 300)           84155700  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 10)            12440     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 49, 5)             105       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 84,168,301\n",
      "Trainable params: 12,591\n",
      "Non-trainable params: 84,155,710\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model.h5'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "model_checkpoint = ModelCheckpoint(best_model_path,\n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='toxic_comments.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 178839 samples, validate on 44710 samples\n",
      "Epoch 1/28\n",
      " - 60s - loss: 0.5660 - val_loss: 0.2996\n",
      "Epoch 2/28\n",
      " - 53s - loss: 0.2135 - val_loss: 0.1078\n",
      "Epoch 3/28\n",
      " - 52s - loss: 0.1098 - val_loss: 0.0722\n",
      "Epoch 4/28\n",
      " - 53s - loss: 0.0877 - val_loss: 0.0633\n",
      "Epoch 5/28\n",
      " - 55s - loss: 0.0791 - val_loss: 0.0602\n",
      "Epoch 6/28\n",
      " - 63s - loss: 0.0748 - val_loss: 0.0575\n",
      "Epoch 7/28\n",
      " - 71s - loss: 0.0718 - val_loss: 0.0567\n",
      "Epoch 8/28\n",
      " - 64s - loss: 0.0703 - val_loss: 0.0556\n",
      "Epoch 9/28\n",
      " - 52s - loss: 0.0689 - val_loss: 0.0549\n",
      "Epoch 10/28\n",
      " - 58s - loss: 0.0678 - val_loss: 0.0555\n",
      "Epoch 11/28\n",
      " - 54s - loss: 0.0666 - val_loss: 0.0539\n",
      "Epoch 12/28\n",
      " - 53s - loss: 0.0655 - val_loss: 0.0536\n",
      "Epoch 13/28\n",
      " - 58s - loss: 0.0651 - val_loss: 0.0531\n",
      "Epoch 14/28\n",
      " - 51s - loss: 0.0645 - val_loss: 0.0538\n",
      "Epoch 15/28\n",
      " - 51s - loss: 0.0637 - val_loss: 0.0526\n",
      "Epoch 16/28\n",
      " - 55s - loss: 0.0633 - val_loss: 0.0526\n",
      "Epoch 17/28\n",
      " - 61s - loss: 0.0626 - val_loss: 0.0528\n",
      "Epoch 18/28\n",
      " - 56s - loss: 0.0621 - val_loss: 0.0526\n",
      "validation AUC 0.9768050304187734\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,validation_data=(X_test, y_test),\n",
    "                 epochs=28, batch_size=BATCH_SIZE, shuffle=True, verbose=2,\n",
    "                 callbacks=[model_checkpoint, early_stopping])\n",
    "model.load_weights(best_model_path)\n",
    "test_pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('validation AUC',roc_auc_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_clean'] = test.comment_text.apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test.comment_clean)\n",
    "test_data = pad_sequences(test_seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                         truncating='post')\n",
    "with open(dataPath + 'test_comments.pkl','wb') as f: pickle.dump(test_data, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_pred = model.predict(test_data, batch_size=BATCH_SIZE, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(test2_pred, index=test.index)\n",
    "out.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "out.to_csv(dataPath + 'tc_output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89186, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
